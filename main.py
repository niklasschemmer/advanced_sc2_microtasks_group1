"""
This main module contains the main part of the program.
Optional parameters can be changed in the code, or passed via command line parameters.
Authors: Dominik Brockmann, Niklas Schemmer
"""
from absl import flags
import sys
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pysc2.env import sc2_env
import scipy.signal
import time
from utils import utils
from pysc2.lib import actions
from buffer.buffer import Buffer
from network.ppo_network import PPO_Network_actor, PPO_Network_critic
import os

FLAGS = flags.FLAGS
flags.DEFINE_integer(
    "steps_per_epoch",
    300,
    "How many steps to do while one epoch iteration.")
flags.DEFINE_integer("epochs", 75, "Number of epochs for training.")
flags.DEFINE_float("gamma", 0.99, "Discount value for future rewards.")
flags.DEFINE_float("clip_ratio", 0.2, "Discount value for future rewards.")
flags.DEFINE_float(
    "policy_learning_rate",
    7e-6,
    "Learning rate for the policy function.")
flags.DEFINE_float("value_function_learning_rate", 1e-5,
                   "Learning rate for the value funtion.")
flags.DEFINE_integer(
    "train_policy_iterations",
    75,
    "Max training iterations for policy funtion.")
flags.DEFINE_integer(
    "train_value_iterations",
    75,
    "Training iterations for value funtion.")
flags.DEFINE_float(
    "lam",
    0.9,
    "Smoothing parameter used for reducing variance.")
flags.DEFINE_float(
    "target_kl",
    0.01,
    "Target Kullback Leibler divergence to stop at if reached while policy update.")
flags.DEFINE_integer("step_mul", 8, "Game steps per agent step.")
flags.DEFINE_integer(
    "screen_resolution",
    64,
    "Resolution for screen feature layers.")
flags.DEFINE_integer(
    "minimap_resolution",
    64,
    "Resolution for minimap feature layers.")
flags.DEFINE_integer("max_agent_steps", 120, "Total agent steps.")

FLAGS(sys.argv)

# Activate GPU usage for tensorflow
physical_devices = tf.config.list_physical_devices('GPU')
try:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
except BaseException:
    pass

# Setup the SC2 environment, to use a different minimap change the minimap
# name and be aware it is located inside your maps folder
env = sc2_env.SC2Env(
    map_name="DefeatZerglingsAndBanelings",
    players=[sc2_env.Agent(sc2_env.Race.protoss)],
    step_mul=FLAGS.step_mul,
    agent_interface_format=sc2_env.AgentInterfaceFormat(
        feature_dimensions=sc2_env.Dimensions(
            screen=FLAGS.screen_resolution,
            minimap=FLAGS.minimap_resolution),
        use_feature_units=True),
    visualize=True)

# Create the buffer with dimensions of screen for observations
buffer = Buffer((utils.screen_channel(),
                 FLAGS.screen_resolution,
                 FLAGS.screen_resolution),
                len(actions.FUNCTIONS),
                FLAGS.steps_per_epoch,
                lam=FLAGS.lam)

# Initializing actor and critic network
actor = PPO_Network_actor(FLAGS)
critic = PPO_Network_critic()

policy_optimizer = keras.optimizers.Adam(
    learning_rate=FLAGS.policy_learning_rate)
value_optimizer = keras.optimizers.Adam(
    learning_rate=FLAGS.value_function_learning_rate)

# Reset the env and preprocess first observation
observation = env.reset()
episode_return = 0
episode_length = 0
screen = np.array(
    observation[0].observation['feature_screen'],
    dtype=np.float32)
screen = np.expand_dims(utils.preprocess_screen(screen), axis=0)


@tf.function
def logprobabilities(logits, a):
    """
    Calculate the logprobabilites.

    Parameter logits: The logit distribution over the actions generated by the network.
    Parameter a: The selected action.
    """
    logprobabilities_all = tf.nn.log_softmax(logits, axis=-1)
    logprobability = tf.reduce_sum(
        tf.one_hot(a, logits.shape[-1]) * logprobabilities_all, axis=-1
    )
    return logprobability


@tf.function
def train_policy(
        screen_buffer,
        avail_actions_buffer,
        non_spatial_action_buffer,
        spatial_action_buffer,
        logprobability_buffer,
        advantage_buffer):
    """
    Training the policy network with the buffer trajectory.
    All buffer variables contain an array of the same size, where each index is a buffered step from the environment.

    Parameter screen_buffer: The buffered screen, that is gotten from the observation of each step.
    Parameter avail_actions_buffer: The available actions at each step.
    Parameter non_spatial_action_buffer: The selected non spatial action.
    Parameter spatial_action_buffer: The selected spatial action.
    Parameter logprobability_buffer: The calculated logprobability at this step, calculated from logits and the selected action, for non spatial and spatial action.
    Parameter advantage_buffer: The advantage that is calculated after each finished trajectory.
    """
    # Split logprobability buffer, because logprobabilities for non spatial
    # and spatial action are saved in one buffer
    non_spatial_logprobability_t, spatial_logprobability_t = tf.split(
        logprobability_buffer, num_or_size_splits=2, axis=1)
    with tf.GradientTape() as non_spatial_tape, tf.GradientTape() as spatial_tape:
        # Forward pass screen inputs and available actions in actor network to
        # get the logits again
        non_spatial_logits, spatial_out_logits = actor(
            screen_buffer, avail_actions_buffer)

        # KL divergence needs to be calculated for non spatial and spatial
        # action at every time simultaneously
        non_spatial_ratio = tf.exp(
            logprobabilities(non_spatial_logits, non_spatial_action_buffer)
            - non_spatial_logprobability_t
        )
        spatial_ratio = tf.exp(
            logprobabilities(spatial_out_logits, spatial_action_buffer)
            - spatial_logprobability_t
        )
        min_advantage = tf.where(
            advantage_buffer > 0,
            (1 + FLAGS.clip_ratio) * advantage_buffer,
            (1 - FLAGS.clip_ratio) * advantage_buffer,
        )

        non_spatial_policy_loss = -tf.reduce_mean(
            tf.minimum(non_spatial_ratio * advantage_buffer, min_advantage)
        )
        spatial_policy_loss = -tf.reduce_mean(
            tf.minimum(spatial_ratio * advantage_buffer, min_advantage)
        )
    non_spatial_policy_grads = non_spatial_tape.gradient(
        non_spatial_policy_loss, actor.trainable_variables)
    spatial_policy_grads = spatial_tape.gradient(
        spatial_policy_loss, actor.trainable_variables)
    policy_optimizer.apply_gradients(
        zip(non_spatial_policy_grads, actor.trainable_variables))
    policy_optimizer.apply_gradients(
        zip(spatial_policy_grads, actor.trainable_variables))

    non_spatial_logits, spatial_out_logits = actor(
        screen_buffer, avail_actions_buffer)
    non_spatial_kl = tf.reduce_mean(
        non_spatial_logprobability_t
        - logprobabilities(non_spatial_logits, non_spatial_action_buffer)
    )
    spatial_kl = tf.reduce_mean(
        spatial_logprobability_t
        - logprobabilities(spatial_out_logits, spatial_action_buffer)
    )
    kl = tf.add(tf.reduce_sum(non_spatial_kl), tf.reduce_sum(spatial_kl))
    return kl


@tf.function
def train_value_function(
        screen_buffer,
        available_actions_buffer,
        return_buffer):
    """
    Training function for the value policy.
    Trained on a whole trajectory.

    Parameter screen_buffer: The preprocessed screen, extracted from the observation of the environment.
    Parameter available_actions_buffer: The available actions in this step.
    Parameter return_buffer: The one-hot available actions at this step.
    """
    with tf.GradientTape() as tape:
        value_loss = tf.reduce_mean(
            (return_buffer -
             critic(
                 screen_buffer,
                 available_actions_buffer)) ** 2)
    value_grads = tape.gradient(value_loss, critic.trainable_variables)
    value_optimizer.apply_gradients(
        zip(value_grads, critic.trainable_variables))


def step_agent(screen, available_actions, available_actions_one_hot):
    """
    Makes a step for the agent.
    It performs a forward pass and selects the highest rated non spatial action from those that remain available.

    Parameter screen: The preprocessed screen, extracted from the observation of the environment.
    Parameter available_actions: The available actions in this step.
    Parameter available_actions_one_hot: The one-hot available actions at this step.
    """
    # Perform a forward step with the given screen and the available actions
    non_spatial_logits, spatial_out_logits = actor(
        screen, available_actions_one_hot)
    # Select a random actions with the probabilities from the logits
    spatial_action = tf.squeeze(
        tf.random.categorical(
            spatial_out_logits, 1), axis=1)

    # Preprocess logits and non spatial action
    non_spatial_logits = non_spatial_logits.numpy().ravel()
    spatial_out_logits = spatial_out_logits.numpy().ravel()
    non_spatial_action = available_actions[tf.random.categorical(
        [non_spatial_logits[available_actions]], 1)[0][0]]

    # Translate spatial action into x,y coordinates
    target = spatial_action.numpy()[0]
    target = [int(target // FLAGS.screen_resolution),
              int(target % FLAGS.screen_resolution)]

    # Build envirnment step function call
    act_args = []
    for arg in actions.FUNCTIONS[non_spatial_action].args:
        if arg.name in ('screen', 'minimap', 'screen2'):
            act_args.append([target[1], target[0]])
        else:
            act_args.append([0])

    # Calculate critic value
    value_t = critic(screen, available_actions_one_hot)
    func_actions = actions.FunctionCall(non_spatial_action, act_args)

    return func_actions, non_spatial_logits, spatial_out_logits, non_spatial_action, spatial_action, value_t


"""
This for loop iteration runs a predefined number of times as epochs and runs the whole training process of the network.
"""
for epoch in range(FLAGS.epochs):
    sum_return = 0
    sum_length = 0
    num_episodes = 0
    num_frames = 0

    # Iterate a certain maximum numbers of times per epoch to create the
    # needed trajectories
    for t in range(FLAGS.steps_per_epoch):
        # Preprocess screen and available actions
        available_actions = observation[0].observation['available_actions']
        available_actions_one_hot = np.zeros(
            [1, len(actions.FUNCTIONS)], dtype=np.float32)
        available_actions_one_hot[0, ] = 1

        # Makes one step with the agents
        func_actions, non_spatial_logits, spatial_out_logits, non_spatial_action, spatial_action, value_t = step_agent(
            screen, available_actions, available_actions_one_hot)
        # Calls the step funtion of the environment
        observation_new = env.step([func_actions])
        episode_return += observation_new[0].reward
        episode_length += 1
        num_frames += 1

        # Calculate the logprobabilities later used for training
        spatial_logprobability_t = logprobabilities(
            spatial_out_logits, spatial_action)
        non_spatial_logprobability_t = logprobabilities(
            non_spatial_logits, non_spatial_action)

        # Store each step item in the buffer
        buffer.store(
            screen,
            available_actions_one_hot,
            non_spatial_action,
            spatial_action,
            observation_new[0].reward,
            value_t,
            (non_spatial_logprobability_t,
             spatial_logprobability_t))

        # Set the old observation to the newly made iteration
        observation = observation_new
        screen = np.array(
            observation[0].observation['feature_screen'],
            dtype=np.float32)
        screen = np.expand_dims(utils.preprocess_screen(screen), axis=0)

        # Terminal state is reached either if environment has ended or if the
        # max agent steps are reached
        terminal = (
            num_frames >= FLAGS.max_agent_steps) or observation[0].last()
        if terminal or (t == FLAGS.steps_per_epoch - 1):
            # Calculate either the last value each epoch end is reached or set
            # to zero
            last_value = 0 if terminal else critic(
                screen, available_actions_one_hot)
            # Finish the trajectory and calculate agvantages
            buffer.finish_trajectory(last_value)
            sum_return += episode_return
            sum_length += episode_length
            num_episodes += 1
            num_frames = 0
            # Resets the environment
            observation, episode_return, episode_length = env.reset(), 0, 0

    # Gets all the generated trajectories from the buffer
    (
        screen_buffer,
        available_actions_buffer,
        non_spatial_action_buffer,
        spatial_action_buffer,
        advantage_buffer,
        return_buffer,
        logprobability_buffer,
    ) = buffer.get()

    # Repeat policy training for a certain amount of times and early stop if a
    # certain target KL value is reached
    for _ in range(FLAGS.train_policy_iterations):
        kl = train_policy(
            screen_buffer,
            available_actions_buffer,
            non_spatial_action_buffer,
            spatial_action_buffer,
            logprobability_buffer,
            advantage_buffer)
        if kl > 1.5 * FLAGS.target_kl:
            break

    # Repeat value function training for a certain amount of times
    for _ in range(FLAGS.train_value_iterations):
        train_value_function(
            screen_buffer,
            available_actions_buffer,
            return_buffer)

    print(
        f" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}"
    )
